![[grandoralsujet1.odt]]
**La génomique, reine du Big data ?**

  

  

  

**Introduction**

_pbq_

_definitions_

  

Je vais vous presenter mon expose de grand oral sur la problematique «La génomique, reine du Big data ?». Alors pourquoi cette question et pourquoi c’est hypothese, avant d’y repondre il est important de bien comprendre les deux sujets de cette problematique en commenceant par le Big Data.

  

  

  

  

**Le Big Data**

  

_big data anglais_

_concept 2000_

_traditionel → moderne_

  

Big Data c’est une expression anglophone qui se traduit literallement par Grosses Donnees ou plutot par les megadonnees voir encore les donnees massives. Le Big Data est un concept qui nait environ au debut des annees 2000 a cause d’une situation bien precise.

  

En fait, avec la hausse continue de production et de stockage de donnees informatiques, scientifiques et autres il devient impossible de traiter ces donnees de maniere traditionelle, lorsqu’on parle de maniere traditionelle on veut dire par exemple avec un seul ordinateur et un seul disque dur. Comme cela n’etait plus possible de nouvelles facons de reflechir le traitement et le stockage des donnees ont ete mise en place et c’est suite a cela que le concept de Big Data a ete invente.

  

  

  

  

  

**Les 5 Grands V**

  

_general_

_5 grands V_

_volume_

_velocitee_

_varietee_

  

_valeur_

_veracitee_

_mini ccl_

  

De facon general ce concept désigne donc d’un cote la production de plus en plus importante de données et de l’autre le développement de technologies capables de les traiter, mais pour mieux comprendre en detail ce qu’il represente vraiment il faut forcement parler des 5 grands V du Big Data. Les trois premiers sont les plus anciens et remontent a 2001;

  

- En premier il y a le **Volume**, c’est l’idee la plus transparente; il y a de plus en plus de donnees et elles representent un volume de plus en plus important.
    

  

- En deuxieme nous avons la **V****elocitee**, c’est a dire que des quantitees enormes de donnes sont generees tres rappidement, on peut penser par exemple a TikTok ou FaceBook ou des millions de personnes like des posts au meme instant pendant toute la journee.
    

  

- Et en troisieme c’est la **Varietee** des donnees, c’est a dire que les informations ne sont plus seulement stockees dans des formats textes ou tableurs mais dans des formats tres variees. On appele ces donnes les donnes non structurees qui peuvent etre par exemple des images des audios ou des videos et qui s’opposent aux donnees structurees plus traditionelles.
    

  

Meme si ces trois points pourraient suffir a comprendre le Big Data, les choses changent tres vite et il me semble important aujourdhui de rajouter deux autres grands V a sa definition.

  

- En quatrieme serait donc la **Valeur**; si les megadonnees n’avaient pas de valeurs, toutes les infrastructures materielles mais aussi les projets de recherche et bien d’autres choses ne pourraient pas exister et meme faire des profits. Pour la SNCF par exemple, les donnees sur les positions de leurs trains sont indispensables pour pouvoir organiser et gerer leurs lignes et ca aussi c’est du Big Data.
    

  

- Enfin le cinquieme V n’est autre que la **V****eracitee**, c’est a dire a quel point la donnee est fiable puisque les sources de production de donnees se multiplient il faut faire le tri et ne pas enregistrer de fausses donnees.
    

  

  

Evidement ces 5 V ne sont pas representatifs de l’entieretee du Big Data mais ils permettent de mieux comprendre ce que cela signifie en allant un peu plus loins. Mais alors quel lien faire avec la genomique et puis tout d’abord c’est quoi la genomique ?

  

  

  

  

  

**Le genome et l’ADN**

  

_intro bases_

_molecule d’ADN_

_genes_

_definition genomique + lien big data_

_transition 5V_

  

Avant d’expliquer ce qu’est vraiment la genomique, il faut d’abord bien comprendre ce qu’est le genome et l’ADN. Comme vous le savez peut etre, les organismes vivants sont composes de cellules, et chaque cellule contient un noyau qui contient son l’ADN (ou acide desoxyribonucleique).

  

L’ADN est une molecule et elle a une structure de double brin, chaque brin est compose d’une suite des quattres briques elementaires appelees nucleotides et symbolisees par les lettres ATCG. Les deux brins d’ADN sont complementaires l’un de l’autre. A chaque A d’un brin correspond un T sur l’autre et a chaque C un G.

  

Mais ce message est tres long et il est donc organise en differentes parties que l’on appel genes, il y en a au total environ 30000 chez l’etre humain et presque tous contiennent la recette en quelque sorte a la production d’une proteine particulliere. Un gene c’est donc un segment de notre ADN et les genes sont tres importants a notre corps et notre santee, par exemple on dit souvent qu’un gene va determiner la couleur de nos yeux mais il faut faire attention car ce sont en realitee plusieurs genes ensembles qui peuvent affecter la couleurs de nos yeux. Et cela nous ammene au genome, le genome c’est non pas seulement un gene mais l’entieretee du materiel genetique d’un organisme, c’est a dire tout les genes mais aussi le reste de notre ADN.

  

Maintenant que cela a ete mis au clair je l’espere, nous pouvons revenir a notre definition de base sur la genomique. La genomique est donc l’etude du fonctionement par exemple d’un organisme a l’echelle du genome entier. C’est a dire qu’on ne s’interesse pas seulement a un gene mais a toute la composition genetique d’un individu et comme vous l’avez surement devine cela fait donc beaucoups de donnees bien plus que lors de comparaison de genes. C’est la qu’est le lien entre genomique et big data, l’ADN est une donnee en soit mais son etude seuls ne releve pas du domaine de la megadonnee c’est l’etude et l’archivage de milliers de genomes qui font entrer l’ADN dans le domaine du Big Data.

  

Maintenant que nous avons compris pourquoi la genomique releve bel et bien du domaine du Big Data il est temps de prouver si celle-ci est en est bel et bien la reine ? Pour cela nous allons revenir a nos 5 grands V mais cette fois ci en se penchant sur la genomique.

  

  

  

  

  

**Volume**

  

_stocker_

_ex : EGA + comparaison biblioteque_

Commenceons par le plus transparent, le volume du stockage informatique des genomes. En effet, peut importe ce que nous voullons faire en genomique le stockage des genomes est indispensable. Pour comprendre comment cela marche je vais prendre un exemple tres concret c’est a dire l’EGA qui est entre autre l’archive europeene des genomes humains. Cette archive detient actuelement 13 millions de gigaoctets pour un peu plus de 3 millions de fichiers. Dit comme ca on mesure que c’est une quantite importante mais il faut bien comprendre que ce type d’archive ne peut exister que sur des centaines de disques durs qui occupent des hangards de la taille de terrains de foot aussi appeles data center. Ce qui est surtout impressionant c’est que l’ADN qui est donc une molecule minuscule represente en realitee une quantitee de donnees absurde qui depasse de tres tres loin nos capacitees de stockage actuelles, si on compare par exemple a la place que prennent les 14 millions de livres de la librairie britanique ceux-ci represente environ 14 000 gigaoctets c’est a dire moins de 0.1% de la taille que represente seulement 1 millions de genomes et pourtant dans les deux cas on ne parle que de suites de lettres.

  

  

  

  

  

**Velocitee**

  

_etudier genomes_

_sequencage d’ADN_

  

A l’inverse lorqu’il s’agit de la velocitee de la genomique les choses sont bien differentes. Concretement pour realiser l’etude d’un genome on peut par exemple le comparer a d’autres genomes comme entre des patients malades et saints pour essayer de reperer a partir des differences entre leurs genomes d’ou pourrait provenir la maladie, mais il va avant tout falloir le lire, et pour lire un genome nous devons sequencer de l’ADN. Le processus de sequencage est une technologie recentes, dans les annees 90 ce processus coutait des centaines de millions d’euros pour un seul genome humain, heureusement les technologies de sequencages d’ADN evoluent a une vitesse sans precedent et aujourdhui le sequencage d’un genome humain ne coute plus que quelques centaines d’euros et se deroule bien plus rappidement qu’il y a a peine 10ans. Mais cela reste un processus long autant dans sa lecture que dans les procedures qui l’entoure. La genomique est donc pour l’instant lente en production et traitement des donnees meme si les choses changent tres vite.

  

  

  

  

**Varietee**

  

_adn variee & conservee_

_almost only adn_

  

Quand a la varietee et bien la encore la genomique n’est certainement pas la plus forte, certe notre ADN est tres variee est c’est encore plus le cas chez les autres etres vivants nous commenceons seulement a s’interesser a l’ADN d’autres especes et apprennons déjà beaucoup. De plus, l’ADN se conserve tres longtemps et aujourdhui encore nous pouvons sequencer des genomes entiers date a plusieurs centaines de milliers d’annees mais il reste important de faire remarquer que la genomique meme s’il est possible d’y lier d’autres domaines scientifiques reste avant tout basee sur le genome et donc des donnees relativement peu variee comparee a tout ce que nous pouvons actuelement stocker informatiquement.

  

  

  

  

**Valeur**

  

_ADN = vie_

_future_

  

Mais il y a bien un domaine ou la genomique est sans aucun doute imbattable et c’est celui de la valeur. Il est difficile de donner un valeur a notre ADN meme tellement elle nous est importante, la genomique s’interesse non seulement a notre ADN mais tente de lui donner un sens et plus enore de venir a bout des maladies les plus grave pour l’etre humain mais aussi pour toute la biodiversitee qui nous entoure. Avec le changement climatique la genomique pourrait permettre de sauver des especes entiere de plantes et d’arbres en comprenant pourquoi certains resistent aux challeurs extremes et d’autres non et en selectionnant petit a petit certains individus des ecosystemes entiers pourraient etre preserver. Alors la valeurs de la genomique est sans aucun doute plus qu’importante mais indispensable et elle le restera aussi longtemps que la vie existera sur terre.

  

  

  

  

**Veracitee**

  

_erreurs humaine + molecule_

  

Enfin, le dernier point est celui de la veracitee et malheureusement la genomique a la encore des proggres a faire. Malgres leurs efforts les sequenceurs actuels par exemple peuvent encore faire des erreurs lors de la lecture, la plupart du temps celles-ci ne sont pas grave et sont meme parfois corrigees mais l’ADN etant une molecule vivante elle est heureusement assez fiable mais reste victime de diverses mutations et autres soucis de fiabilitee au cours de son existence et de sa lecture.

  

  

  

  

**Conclusion**

  

_not reine but important_

_IA + future_

  

Alors certe la genomique n’est pas parfaite et elle n’est pas non plus la reine du big data, il est de toute facon difficile de placer quelque chose en haut de ce domaine recent et en constante evolution. Il est certain que la genomique joue un rôle majeur et participe d’ailleurs toujours a l’evolution du monde du Big Data en ce moment on parle enormement d’IA ces algorithmes complexes font justement partis du future de la genomique et sont déjà un petit peu utilise en recherche pour accelerer et automatiser certaines taches de comparaisons et autre. Finallement la genomique n’est peut etre pas reine de l’entieretee du Big Data mais elle joue certainement un rôle important et qui sait ce que le future peut nous reserver. Merci de m’avoir ecoute!

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

Cependant, actuelement le sequencage d’un genome se fait d’une maniere un petit peu particuliere, du entre autre a la faible vitesse de lecture des sequenceurs le genome lu va etre separe en des milliers de petites parties puis va etre reconstitue dans son ensemble a l’aide de programmes informatiques. La lecture d’un genome est donc un processus informatise qui utilise des programmes complexes melants a la fois biologie chimie informatique et mathematiques afin de pouvoir lire et comprendre les genomes. Au dela de la lecture il y a en effet un tout autre processus de classification et de disection du genome pour identifier des parties utiles liees aux systemes de defenses de l’orgnanisme par exemple et aux parties moins utiles par exemples les genes en charge de la couleurs de yeux. Ce processus est long et requiert des systemes et des serveurs d’une certaines puissance et s’entremelle d’ailleurs tres bien avec ce que l’on appelle l’intelligence artificielle qui commence déjà et continuera tres probablement a travailler sur les genomes dans le but de mieux les comprendre et les analyser mais surtout d’augmenter les vitesses des analyses.

  

  

Au dela du volume de l’ADN son stockage fait aussi debat, de nombreuses archives sont semi-publiques ce qui veut dire qu’elles se partagent au seins des scientifiques et chercheurs du monde entier mais aussi qu’il y a des risques de securites avec leur stockage en particuliers lorsque l’on parle d’ADN humaine et c’est la que notre 2eme sujet arrive, l’ADN est certe une donnee importante, volumineuse, dense, variee, produite en continue, en constante evolution depuis des millions d’annees mais qu’en est-il de son utilisation?

  

Concretement pour realiser l’etude d’un genome on peut par exemple le comparer a d’autres genomes comme entre des patients malades et saints pour essayer de reperer a partir des differences entre leurs genomes d’ou pourrait provenir la maladie, mais il va avant tout falloir le lire, pour faire cela nous devons sequencer de l’ADN. Ce qui est important a savoir a propos du sequencage c’est que c’est une technologie relativement recente, c’est un projet americain appele the human genome qui marque le tout debut du sequencage d’ADN, a l’epoque ce processus coutait des centaines de millions d’euros pour un seul genome humain, heureusement les technologies de sequencages d’ADN evoluent a une vitesse sans precedent et aujourdhui le sequencage d’un genome humain ne coute plus que quelques centaines d’euros et se deroule bien plus rappidement qu’il y a a peine 10ans. Cependant, actuelement le sequencage d’un genome se fait d’une maniere un petit peu particuliere, du entre autre a la faible vitesse de lecture des sequenceurs le genome lu va etre separe en des milliers de petites parties puis va etre reconstitue dans son ensemble a l’aide de programmes informatiques. La lecture d’un genome est donc un processus informatise qui utilise des programmes complexes melants a la fois biologie chimie informatique et mathematiques afin de pouvoir lire et comprendre les genomes. Au dela de la lecture il y a en effet un tout autre processus de classification et de disection du genome pour identifier des parties utiles liees aux systemes de defenses de l’orgnanisme par exemple et aux parties moins utiles par exemples les genes en charge de la couleurs de yeux. Ce processus est long et requiert des systemes et des serveurs d’une certaines puissance et s’entremelle d’ailleurs tres bien avec ce que l’on appelle l’intelligence artificielle qui commence déjà et continuera tres probablement a travailler sur les genomes dans le but de mieux les comprendre et les analyser mais surtout d’augmenter les vitesses des analyses.

  

  

**Critique**

  

Finallement, c’est interessant d’apporter le sujet des IA car c’est peut etre a cela que se heurte la genomique dans sa course au trone du Big Data. Nous avons vu que ce domaine est imbattable sur plusieurs points mais pas sur tous, la genomique est certe un sujet important a nous tous puisqu’elle concerne la vie tout court mais le Big Data c’est avant tout la quantite certe mais la varietee des donnees et face a des modeles d’IA se basant sur des videos, des photos, tout les ecrits possible de trouver sur internet, la genomique elle n’a que le code source de la vie. La genomique doit prendre en compte

  

**Conclusion**

  

Commenceons

  

  

**La genomique**

  

  

  

  

Et c’est la que la genomique entre dans le domaine du Big Data, en effet presque depuis ses debuts la genomique rentre dans presque toutes les cases de ce que l’on appel Big Data. C’est une donnee volumineuse puisque l’ADN a une des densitees de stockage les plus importantes que nous connaissions, elle est bien sur variee, sa valeure est inclassable tant son rôle est indispensable a la survie de notre planete toute entiere, et sa veracitee n’est pas negligeable. Mais finnalement c’est surtout l’evolution des technologies de sequencages d’ADN entre autre qui font peut-etre d’elle la reine du big data ?

  

  

  

  

  

Vous l’avez peut etre devinne cette evolution du coup et du temps de sequencage a permis de sequencer beaucoup plus de genomes qu’ils soient humains ou non et de les etudier puis les archiver dans d’enormes bases de donnees.

  

  

  

  

  

comprendre ce que signifie la big data et pourquoi faire un lien avec la genetique

  

expliquer déjà pourquoi l’ADN est une donnee et comment on la lit etc

  

montrer la transformation de la genomique en qqc qui releve de la big data

  

expliquer les recherches actuels en genomiques

  

conclusion; not queen mais certainement un domaine majeur et prometteur de la big data

  

  

  

  

  

key concepts :

  

advancement in tech made it possible for dna to join the big data world

  

  

  

  

Big Data désigne à la fois la production de

données massives et le développement de technologies capables de les traiter

afin d’en extraire des corrélations ou du sens v

  

  

  

  

  

  

  

Depuis la fin des années 1990 et le projet « génome humain », les technologies

de séquençage de l’ADN connaissent une véritable révolution qui fait aujourd’hui

entrer la génomique dans l’ère du Big Data.

  

D epuis une quinzaine d’années, le volume

des données générées dans le domaine

de la génomique a crû en proportion

inverse du coût du séquençage [1] : ainsi,

le plan France Médecine Génomique 2025, annoncé

en 2016, prévoit de produire plusieurs dizaines de Po

de données par an d’ici 5 ans. Leur stockage, leur

accessibilité et leur exploitation sont donc des

problématiques à part entière. Trois domaines sont

particulièrement concernés : l’exploration de la

diversité du vivant et des écosystèmes complexes, la

génomique fonctionnelle (voir encadrés) et la méde-

cine génomique de précision, qui implique la re-

cherche de biomarqueurs par l’analyse d’ensembles

de données hétérogènes. La question du volume des

données est par ailleurs indissociable de celle de leur

accumulation au cours de projets s’étalant sur plu-

sieurs années : dans le cas de la médecine de préci-

sion, c’est l’enrichissement des bases de données,

avec des données de séquences, phénotypiques,

environnementales, médicales, et leur confrontation

permanente avec celles précédemment acquises qui

permet, par un mécanisme d’auto-apprentissage,

d’affiner les diagnostics et d’identifier des biomar-

queurs originaux.

Pour le traitement des données, deux grands types

d’usages ont de gros besoins en calcul : l’assemblage

(la reconstitution de novo de génomes à partir de

données brutes de séquençage) et la comparaison de

séquences avec celles déjà connues (Fig. 1). Ces deux

approches ont en partage l’importante quantité de

données à manipuler et la quasi-impossibilité de

calibrer le temps d’exécution. Il s’agit de To de don-

nées et de fichiers de plusieurs dizaines ou centaines

de Go, ce qui impose qu’elles soient au plus près des

moyens de calcul, avec des capacités en lecture/écri-

ture adaptées [2]. Pour les projets génomiques

d’envergure, l’utilisation des technologies du cloud

computing en coordination avec les systèmes HPC

bien connu du monde de la physique ou plus préci-

sément leur évolution HTC permettra de bénéficier

des points forts de ces différents modèles, à savoir

la performance en termes d’entrées/sorties et de

calcul pour le HTC, et les caractéristique d’élasticité et

d’adaptation à la demande du cloud computing [3].

Pour finir, revenons sur deux enjeux essentiels en

génomique humaine. En premier lieu, le partage des

données : étant donnée la rareté des événements

génétiques, les données générées pour un individu

ne prennent de sens et de valeur que si elles peuvent

être croisées avec de nombreuses bases de données,

idéalement avec l’ensemble des génomes connus [4].

En second lieu, la confidentialité des données : rien

n’est plus identifiant que la séquence génomique

d’un individu qui porte des informations prédictives,

et même au-delà des informations touchant ses des-

cendants, ascendants ou sa fratrie. Cet aspect reste

critique et des solutions élégantes émergent, comme

le chiffrement homomorphe, qui permet de calculer

sur une donnée chiffrée et d’obtenir un résultat lui-

même chiffré.

  

  

BOOM FUSION WSHWSH

  

  

  

  

  

  

Liens:

  

[https://www.cea.fr/multimedia/Documents/publications/clefs-cea/CLEFS64-BIGDATA.pdf](https://www.cea.fr/multimedia/Documents/publications/clefs-cea/CLEFS64-BIGDATA.pdf)

  

[https://www.pasteur.fr/fr/file/18077/download](https://www.pasteur.fr/fr/file/18077/download)

  

[https://theconversation.com/la-genomique-reine-du-big-data-84155](https://theconversation.com/la-genomique-reine-du-big-data-84155)

  

[https://www.pasteur.fr/fr/journal-recherche/dossiers/comment-big-data-revolutionne-recherche-sante](https://www.pasteur.fr/fr/journal-recherche/dossiers/comment-big-data-revolutionne-recherche-sante)

  

[https://www.youtube.com/watch?v=kyMzPwS88F8](https://www.youtube.com/watch?v=kyMzPwS88F8)

  

[https://www.youtube.com/watch?v=UFnoxLN17ko](https://www.youtube.com/watch?v=UFnoxLN17ko)